---
layout: post
title: "AWS"
published: true
---

- **large language model (LLM)**: focus on text & language tasks
- **foundational model (FM)**: broader then LLM, can handle various data types
- **prompt engineering**: optimize FM inputs to generate better responses
- **retrieval augmented generation (RAG)**: technique to let LLM use information from external knowledge base
- **continued pre-training**: provide unlabeled data to FM to improve domain knowledge
- **instruction-based fine-tuning**: use labeled examples (prompt, response pairs) to improve FM on a specific task
